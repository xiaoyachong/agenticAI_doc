# Complete Timing Analysis: Profiler vs Logs

## Overview
This document explains the discrepancy between Python profiler timing (0.043s) and actual wall clock timing (24.21s) for the classification node.

---

## Wall Clock Time (From Logs)

**Total Pipeline Time: 43 seconds**

```
â”œâ”€ Task Extraction:      2.13s  (5%)
â”œâ”€ Classification:      24.21s  (56%) ðŸ”¥ BOTTLENECK
â”œâ”€ Orchestration:       10.59s  (25%) ðŸ”¥ BOTTLENECK  
â”œâ”€ Motor Read:           ~2s    (5%)
â””â”€ Response:             ~3s    (7%)
```

---

## Classification Timeline (From Logs)

**Start:** 10:53:06  
**End:** 10:53:30  
**Duration:** 24.21 seconds

### Detailed Capability Checks:
```
10:53:06 - Classifier starts
10:53:08 - Capability 'memory' >>> False
10:53:10 - Capability 'time_range_parsing' >>> False  
10:53:17 - Capability 'python' >>> True
10:53:19 - Capability 'motor_position_read' >>> True
10:53:21 - Capability 'motor_position_set' >>> False
10:53:22 - Capability 'detector_image_capture' >>> False
10:53:24 - Capability 'photogrammetry_scan_execute' >>> False
10:53:26 - Capability 'reconstruct_object' >>> False
10:53:28 - Capability 'ply_quality_assessment' >>> False
10:53:30 - Capability 'display_object' >>> False
10:53:30 - "Completed... in 24.21s"
```

**Analysis:** Each capability check takes ~2 seconds (LLM API call)  
**Total checks:** 10 capabilities Ã— ~2 seconds = ~20-24 seconds

---

## Python Profiler Time (cProfile)

**Total Profiled Time: 71.5 seconds**

```
â”œâ”€ Actual Python execution: ~5 seconds
â”œâ”€ Waiting (kqueue/select): 66.7s (network I/O)
â””â”€ Key functions:
   â€¢ motor_position_read.execute:  2.356s (API call)
   â€¢ bolt_api.get_current_angle:   2.355s (HTTP request)
   â€¢ classification.execute:       0.043s (Python code only)
   â€¢ orchestration.execute:        0.039s (Python code only)
```

### Profiler Output:
```python
ncalls  tottime  percall  cumtime  percall filename:lineno(function)
    11    0.000    0.000    0.043    0.004 classification_node.py:136(execute)
```

---

## The Key Insight

### Why the Massive Difference?

**Python profiler (cProfile) measures:**
- CPU time executing Python code
- Only the time your Python functions are actively running

**Logs measure:**
- Wall clock time (real elapsed time)
- Includes all waiting time for external resources

**LLM API calls:**
- Spend most time waiting for network responses
- Very little Python execution time
- Network I/O appears as `select.kqueue` waiting time

### The Breakdown:
```
classification_node.execute() actual work:     0.043s (Python)
Waiting for 10 LLM API responses:            24.170s (Network)
                                            â”€â”€â”€â”€â”€â”€â”€â”€â”€
Total wall clock time:                       24.213s
```

---

## Conclusion

The profiler shows **0.043s** but logs show **24.21s** because:

1. **0.043s** = Pure Python code execution time in the classification node
2. **24.17s** = Network waiting time for LLM API responses
3. **24.21s** = Total wall clock time (0.043 + 24.17)

The difference (~24 seconds) is entirely network I/O waiting time, which doesn't count as "Python execution time" in cProfile but does count in real-world elapsed time.

---

## Bottleneck Summary

**Primary Bottlenecks:**
1. **Classification (24.21s):** 10 sequential LLM API calls
2. **Orchestration (10.59s):** Additional LLM processing

**Total LLM waiting time:** ~35 seconds out of 43 seconds (81% of pipeline)

**Recommendation:** Consider parallelizing capability checks or implementing caching for common task patterns.

# Classification Node - Complete Analysis

## What Classification Does

Classification analyzes the user's task and determines which capabilities are needed to fulfill it.

**Input:**
```python
Task: "Read the motor position"
Available Capabilities: [
    'memory', 'time_range_parsing', 'python', 'motor_position_read',
    'motor_position_set', 'detector_image_capture', 
    'photogrammetry_scan_execute', 'reconstruct_object',
    'ply_quality_assessment', 'display_object', 'respond', 'clarify'
]
```

**Output:**
```python
Active Capabilities: ['respond', 'clarify', 'python', 'motor_position_read']
```

## Why It Checks Each Capability

### The Code Flow:

From `classification_node.py` line 240-280:
```python
async def select_capabilities(
    task: str,
    available_capabilities: List[BaseCapability],
    state: AgentState,
    logger,
    previous_failure: Optional[str] = None
) -> List[str]:
    
    registry = get_registry()
    always_active_names = registry.get_always_active_capability_names()
    
    active_capabilities: List[str] = []
    
    # Step 1: Add always-active capabilities (no LLM needed)
    for capability in available_capabilities:
        if capability.name in always_active_names:
            active_capabilities.append(capability.name)
    
    # Step 2: Classify remaining capabilities (LLM call for each)
    remaining_capabilities = [
        cap for cap in available_capabilities 
        if cap.name not in always_active_names
    ]
    
    # âš ï¸ SEQUENTIAL LOOP - One LLM call per capability
    for capability in remaining_capabilities:
        is_required = await _classify_capability(
            capability, task, state, logger, previous_failure
        )
        
        if is_required:
            active_capabilities.append(capability.name)
    
    return active_capabilities
```

### Why Check Each Capability Individually?

Each capability needs separate analysis because:

1. **Different Context** - Each capability has unique:
   - Instructions (what it does)
   - Examples (when to use it)
   - Requirements (what it needs)

2. **Independent Decisions** - Checking if `motor_position_read` is needed is completely different from checking if `python` is needed

3. **Few-Shot Learning** - Each LLM call includes capability-specific examples

## The LLM Call (Per Capability)

### Line that Calls LLM:

`classification_node.py` line 285-340:
```python
async def _classify_capability(
    capability: BaseCapability, 
    task: str, 
    state: AgentState, 
    logger, 
    previous_failure: Optional[str] = None
) -> bool:
    
    # Get capability-specific instructions and examples
    classifier = capability.classifier_guide
    capability_instructions = classifier.instructions
    examples_string = ClassifierExample.format_examples_for_prompt(
        classifier.examples
    )
    
    # Build prompt
    prompt_provider = get_framework_prompts()
    classification_builder = prompt_provider.get_classification_prompt_builder()
    system_prompt = classification_builder.get_system_instructions(
        capability_instructions=capability_instructions,
        classifier_examples=examples_string,
        context=None,
        previous_failure=previous_failure
    )
    message = f"{system_prompt}\n\nUser request:\n{task}"
    
    # âš ï¸ THE LLM CALL (blocking, ~2 seconds each)
    response_data = await asyncio.to_thread(
        get_chat_completion,
        model_config=get_model_config("framework", "classifier"),
        message=message,
        output_model=CapabilityMatch,  # Returns {is_match: true/false}
    )
    
    logger.info(f" >>> Capability '{capability.name}' >>> {response_data.is_match}")
    return response_data.is_match
```

## Input to LLM (Per Capability)

### Example for `motor_position_read`:
```
[System Instructions]
You are a task classifier. Determine if this capability matches the user's request.

Capability: motor_position_read
Description: Read current motor angle from hardware
Provides: MOTOR_POSITION context

Examples of when this capability IS needed:
- "What's the current position?"
- "Read the motor angle"
- "Check where the motor is"

Examples of when this capability is NOT needed:
- "Move to 45 degrees" (this is motor_position_SET, not read)
- "Take an image" (different capability)

Return a JSON response: {"is_match": true/false}

User request:
Read the motor position
```

**LLM Output:**
```json
{
  "is_match": true
}
```

## Does It Use Parallel Processing?

**NO - It's Sequential! âŒ**

From the code:
```python
# âš ï¸ SEQUENTIAL - One at a time
for capability in remaining_capabilities:
    is_required = await _classify_capability(...)  # Waits for each
    if is_required:
        active_capabilities.append(capability.name)
```

### Your Execution Timeline:
```
10:53:08 - 'memory' >>> False                    [2s]
10:53:10 - 'time_range_parsing' >>> False        [2s]
10:53:17 - 'python' >>> True                     [7s] âš ï¸
10:53:19 - 'motor_position_read' >>> True        [2s]
10:53:21 - 'motor_position_set' >>> False        [2s]
10:53:22 - 'detector_image_capture' >>> False    [1s]
10:53:24 - 'photogrammetry_scan_execute' >>> False [2s]
10:53:26 - 'reconstruct_object' >>> False        [2s]
10:53:28 - 'ply_quality_assessment' >>> False    [2s]
10:53:30 - 'display_object' >>> False            [2s]

Total: 24.21 seconds (10 sequential LLM calls)
```

## Why Not Parallel?

**Current Implementation:**
```python
# Sequential
for capability in remaining_capabilities:
    result = await _classify_capability(...)  # One at a time
```

**Could Be Parallel:**
```python
# Parallel - All at once!
tasks = [
    _classify_capability(cap, task, state, logger, previous_failure)
    for cap in remaining_capabilities
]
results = await asyncio.gather(*tasks)  # All in parallel

# Would take ~2 seconds total instead of 24 seconds!
```

**Why It's Not Parallel (Speculation):**
1. **Rate Limiting** - Avoid hitting LLM API rate limits
2. **Cost Control** - Track/limit concurrent requests
3. **Simplicity** - Sequential code is easier to debug
4. **Historical** - Might have been parallel before, changed for stability

## Does LangGraph Have Quick Classification Logic?

**NO - LangGraph is Just the Execution Framework**

LangGraph provides:
- âœ… Node execution
- âœ… State management
- âœ… Routing between nodes
- âœ… Checkpointing
- âŒ NO built-in classification logic

**What You Defined in LangGraph:**
```python
# In graph_builder.py
workflow = StateGraph(AgentState)
workflow.add_node("classifier", ClassificationNode.langgraph_node)
workflow.add_edge("task_extraction", "classifier")
```

This just tells LangGraph:
- "There's a node called 'classifier'"
- "Run ClassificationNode.execute() when you reach it"
- "Route to it after task_extraction"

The actual classification logic (LLM calls, capability checking) is in your code (`classification_node.py`), not in LangGraph!

## Optimization Opportunities

### 1. Make It Parallel ðŸš€

```python
async def select_capabilities(
    task: str,
    available_capabilities: List[BaseCapability],
    state: AgentState,
    logger,
    previous_failure: Optional[str] = None
) -> List[str]:
    
    # Always-active capabilities (no LLM)
    always_active_names = registry.get_always_active_capability_names()
    active_capabilities = [
        cap.name for cap in available_capabilities 
        if cap.name in always_active_names
    ]
    
    # Remaining capabilities need classification
    remaining = [
        cap for cap in available_capabilities 
        if cap.name not in always_active_names
    ]
    
    # âœ… PARALLEL - All at once!
    classification_tasks = [
        _classify_capability(cap, task, state, logger, previous_failure)
        for cap in remaining
    ]
    
    results = await asyncio.gather(*classification_tasks)
    
    # Add matched capabilities
    for cap, is_required in zip(remaining, results):
        if is_required:
            active_capabilities.append(cap.name)
    
    return active_capabilities
```

**Result: 24 seconds â†’ 2-3 seconds (10x faster!)**

### 2. Cache Common Classifications

```python
# Cache for simple queries
CLASSIFICATION_CACHE = {
    "read motor position": ['motor_position_read', 'respond'],
    "take image": ['detector_image_capture', 'respond'],
    "move to 45": ['motor_position_set', 'respond']
}

if task.lower() in CLASSIFICATION_CACHE:
    return CLASSIFICATION_CACHE[task.lower()]  # Instant!
```

### 3. Batch Classification (Advanced)

```python
# Single LLM call for all capabilities
message = f"""
Classify which capabilities are needed for this task.

Task: {task}

Capabilities:
1. motor_position_read - Read motor angle
2. motor_position_set - Move motor  
3. detector_image_capture - Take image
... (all 10 capabilities)

Return JSON: {{"motor_position_read": true, "motor_position_set": false, ...}}
"""

# One call instead of 10!
result = await get_chat_completion(message, output_model=ClassificationResult)
```

### 4. Early Termination

```python
# Stop checking once you have enough capabilities
MINIMUM_CAPABILITIES = ['respond']  # Always need at least respond

for capability in remaining_capabilities:
    is_required = await _classify_capability(...)
    if is_required:
        active_capabilities.append(capability.name)
        
    # If we have a domain capability + respond, we're done!
    if len(active_capabilities) >= 2 and 'respond' in active_capabilities:
        logger.info("Found sufficient capabilities, stopping early")
        break
```

## Summary Table

| Aspect | Current Implementation | Why | Optimization |
|--------|----------------------|-----|--------------|
| Sequential vs Parallel | Sequential âŒ | Simplicity, rate limiting | Make parallel â†’ 10x faster |
| Number of LLM Calls | 10 calls (one per capability) | Each needs custom context | Batch into 1 call or cache |
| Time Per Call | ~2 seconds | Network + LLM processing | Use faster model (GPT-3.5) |
| Total Time | 24.21 seconds | 10 Ã— 2s sequential | â†’ 2-3s with parallel |
| Always-Active | Skips LLM for respond, clarify âœ… | Smart optimization | Already optimized |
| LangGraph Role | Just executes the node | Framework only | No built-in classification |
| Caching | None âŒ | Not implemented | Cache common queries |

## Key Findings

**ðŸ”¥ Biggest Bottleneck:**
Sequential classification takes 24 seconds because it makes 10 LLM calls one-by-one instead of parallel.

**âœ… Already Optimized:**
- `respond` and `clarify` are "always-active" (no LLM call needed)
- Uses structured output (CapabilityMatch) for reliable parsing

**ðŸš€ Quick Win:**
Make the classification calls parallel using `asyncio.gather()`:
```python
results = await asyncio.gather(*[
    _classify_capability(cap, ...) 
    for cap in remaining_capabilities
])
```
This single change would reduce 24 seconds â†’ 2-3 seconds!

---

# What Orchestration Does

Orchestration transforms classified capabilities into a detailed, executable step-by-step plan with data flow.

## The Complete Picture

**Input to Orchestrator:**
```python
Task: "Read the motor position"
Active Capabilities: ['respond', 'clarify', 'python', 'motor_position_read']
Available Context: {} (empty for new queries)
```

**Output from Orchestrator:**
```python
ExecutionPlan with 2 steps:

Step 1:
  â”œâ”€ context_key: "current_motor_position"
  â”œâ”€ capability: "motor_position_read"
  â”œâ”€ task_objective: "Read the current position of the sample rotation 
  â”‚                   motor to determine its angle in degrees"
  â”œâ”€ success_criteria: "Motor position successfully retrieved with angle value"
  â”œâ”€ expected_output: "MOTOR_POSITION"
  â”œâ”€ parameters: None
  â””â”€ inputs: []

Step 2:
  â”œâ”€ context_key: "motor_position_response"  
  â”œâ”€ capability: "respond"
  â”œâ”€ task_objective: "Provide the user with the current motor position 
  â”‚                   information including motor ID, angle, and timestamp"
  â”œâ”€ success_criteria: "User receives clear response with motor position"
  â”œâ”€ expected_output: None
  â”œâ”€ parameters: None
  â””â”€ inputs: [{"MOTOR_POSITION": "current_motor_position"}]  # Data from Step 1
```

## What the Orchestrator Figures Out

The orchestrator doesn't just pick capabilities - it creates a rich execution plan with:

### 1. Execution Order
- Which capability runs first, second, etc.
- Ensures dependencies are satisfied

### 2. Detailed Task Objectives
- Not just "run motor_position_read"
- But "Read the current position of the sample rotation motor to determine its angle in degrees"
- Each step gets a clear, specific goal

### 3. Data Flow Between Steps
```
Step 1 produces â†’ MOTOR_POSITION context
              â†“
Step 2 consumes â† {"MOTOR_POSITION": "current_motor_position"}
```
- Connects outputs from one step to inputs of the next
- Ensures data is available when needed

### 4. Success Criteria
- How to know if each step succeeded
- Used for error handling and retries

### 5. Context Keys
- Unique identifiers for storing results
- "current_motor_position" â†’ stores motor reading
- "motor_position_response" â†’ stores final response

## Why It Needs an LLM

### The Challenge:

The orchestrator receives just:
- A list of capability names: `['motor_position_read', 'respond', 'python', 'clarify']`
- A user task: `"Read the motor position"`

**What It Must Create:**
- âœ… Detailed step objectives - What should each step accomplish?
- âœ… Execution order - Which order makes sense?
- âœ… Data connections - Which step needs output from which other step?
- âœ… Context keys - How to name and store intermediate results?
- âœ… Success criteria - How to validate each step?

This requires natural language understanding and reasoning - perfect for an LLM!

## The LLM Call

### Line in Code:

`src/framework/infrastructure/orchestration_node.py` ~line 380-390:
```python
execution_plan = await asyncio.to_thread(
    get_chat_completion,
    message=message,
    model_config=model_config,
    output_model=ExecutionPlan  # Returns structured JSON
)
```

### Input Message:
```
[System Instructions]
You are an orchestrator. Create execution plans.

Available capabilities:
- motor_position_read: Read motor angle from hardware
  - Provides: MOTOR_POSITION context
  - Examples: [7 example plans showing how to use this capability]
  
- respond: Generate user response
  - Requires: Context from previous steps
  - Examples: [examples of response plans]

- python: Execute Python code
- clarify: Ask clarification questions

[Rules for creating plans]
- Each step needs: capability, task_objective, inputs, outputs
- Steps must execute in logical order
- Connect data flow between steps
- Final step should be 'respond' or 'clarify'

TASK TO PLAN: Read the motor position
```

### Output (Structured JSON):
```json
{
  "steps": [
    {
      "context_key": "current_motor_position",
      "capability": "motor_position_read",
      "task_objective": "Read the current position of the sample rotation motor to determine its angle in degrees",
      "success_criteria": "Motor position successfully retrieved with angle value",
      "expected_output": "MOTOR_POSITION",
      "inputs": []
    },
    {
      "context_key": "motor_position_response",
      "capability": "respond", 
      "task_objective": "Provide the user with the current motor position information including motor ID, angle in degrees, and timestamp",
      "success_criteria": "User receives clear response with motor position",
      "inputs": [{"MOTOR_POSITION": "current_motor_position"}]
    }
  ]
}
```

**Time: 10.55 seconds** (from your logs)

## Why Not Hardcode?

### Simple Query Example:
"Read motor position" could be hardcoded:
```python
if query matches "read motor":
    return [
        {"capability": "motor_position_read"},
        {"capability": "respond"}
    ]
```

### But Complex Query Example:
"Take images every 10 degrees from 0 to 180, then create a 3D model and assess quality"

The orchestrator must:
1. Break down the complex task into 20+ steps
2. Create loops (repeat image capture 18 times)
3. Track state (current angle position)
4. Chain operations (images â†’ 3D model â†’ quality assessment)
5. Connect data flow (scan results â†’ reconstruction â†’ assessment â†’ response)

**LLM Output:**
```python
{
  "steps": [
    # Step 1-18: Move motor and capture images
    {"capability": "motor_position_set", "parameters": {"angle": 0}},
    {"capability": "detector_image_capture"},
    {"capability": "motor_position_set", "parameters": {"angle": 10}},
    {"capability": "detector_image_capture"},
    # ... (repeat 16 more times)
    
    # Step 19: Reconstruct 3D model from all images
    {"capability": "photogrammetry_scan_execute", 
     "inputs": [{"DETECTOR_IMAGE": "image_0"}, ..., {"DETECTOR_IMAGE": "image_17"}]},
    
    # Step 20: Assess quality
    {"capability": "ply_quality_assessment",
     "inputs": [{"RECONSTRUCT_OBJECT": "3d_model"}]},
    
    # Step 21: Respond with results
    {"capability": "respond",
     "inputs": [{"PLY_QUALITY_ASSESSMENT": "quality_report"}]}
  ]
}
```

This complex planning is why an LLM is needed!

## Optimization Ideas

### 1. Cache Common Patterns
```python
# Skip LLM for simple patterns
SIMPLE_PATTERNS = {
    "read motor position": [
        {"capability": "motor_position_read"},
        {"capability": "respond"}
    ],
    "take image": [
        {"capability": "detector_image_capture"},
        {"capability": "respond"}
    ]
}

if query in SIMPLE_PATTERNS:
    return SIMPLE_PATTERNS[query]  # Instant!
else:
    return await call_llm_orchestrator()  # 10s
```

### 2. Use Faster Model
```python
# GPT-4: 10s, very detailed
# GPT-3.5-turbo: 2-3s, good enough for most plans
model_config = {
    "provider": "openai",
    "model": "gpt-3.5-turbo",  # Faster!
    "temperature": 0.0
}
```

### 3. Parallel Planning (Advanced)
```python
# Run classification and rough planning in parallel
classification_result, draft_plan = await asyncio.gather(
    classify_task(query),
    create_rough_plan(query)  # Quick pattern matching
)
# Then refine draft_plan with LLM if needed
```

## Summary

| Aspect | Details |
|--------|---------|
| Input | Task + Classified capabilities |
| Output | Detailed execution plan with 2+ steps |
| LLM Call | Line 380-390 in orchestration_node.py |
| Time | 10.55 seconds in your example |
| What LLM Creates | Step objectives, data flow, success criteria, execution order |
| Why LLM Needed | Natural language â†’ structured plan with reasoning |
| Model | Configured in model_config (likely GPT-4 or Claude) |
| Optimization | Cache simple patterns, use faster models, or parallel processing |

The orchestrator is essentially a "business analyst" that takes vague requirements (capabilities list + user task) and creates a detailed project plan that engineers (capabilities) can execute step-by-step!

# Complete Logs 
```
================================================================================
BOLT INTERACTIVE PROFILING SESSION
Date: October 13, 2025
Command: read the motor position
================================================================================

SESSION INITIALIZATION
--------------------------------------------------------------------------------
(alpha) xiaoyachong@Mac alpha_berkeley % ./profile_bolt_interactive.sh
ðŸ” Starting BOLT profiling...
Commands to try:
  - What's the current position?
  - Move to 90 degrees
  - Take an image
  - exit (to finish)

Using default value for 'execution_control.limits.max_planning_attempts' = 2


    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘                                                                 â•‘
    â•‘                                                                 â•‘
    â•‘  â–‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–‘â–ˆâ–ˆâ•—â–‘â–‘â–‘â–‘â–‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–‘â–ˆâ–ˆâ•—â–‘â–‘â–ˆâ–ˆâ•—â–‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–‘  â–‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–‘â–ˆâ–ˆâ•—â–‘â–‘â–‘â–‘â–‘â–ˆâ–ˆâ•—  â•‘
    â•‘  â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–‘â–‘â–‘â–‘â–‘â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–‘â–‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—  â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–‘â–‘â–‘â–‘â–‘â–ˆâ–ˆâ•‘  â•‘  
    â•‘  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–‘â–‘â–‘â–‘â–‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–‘â–‘â•šâ•â•â–ˆâ–ˆâ•‘â–‘â–‘â–‘â–‘â–‘â–ˆâ–ˆâ•‘  â•‘
    â•‘  â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–‘â–‘â–‘â–‘â–‘â–ˆâ–ˆâ•”â•â•â•â•â–‘â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–‘â–‘â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–‘â–‘â–‘â–‘â–‘â–ˆâ–ˆâ•‘  â•‘
    â•‘  â–ˆâ–ˆâ•‘â–‘â–‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–‘â–‘â–‘â–‘â–‘â–ˆâ–ˆâ•‘â–‘â–‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–‘â–‘â–ˆâ–ˆâ•‘  â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â•‘
    â•‘  â•šâ•â•â–‘â–‘â•šâ•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â–‘â–‘â–‘â–‘â–‘â•šâ•â•â–‘â–‘â•šâ•â•â•šâ•â•â–‘â–‘â•šâ•â•  â–‘â•šâ•â•â•â•â•â–‘â•šâ•â•â•â•â•â•â•â•šâ•â•  â•‘
    â•‘                                                                 â•‘
    â•‘                                                                 â•‘
    â•‘     Command Line Interface for the Alpha Berkeley Framework     â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


FRAMEWORK INITIALIZATION
--------------------------------------------------------------------------------
ðŸ’¡ Type 'bye' or 'end' to exit

ðŸ”„ Initializing configuration...
ðŸ”„ Initializing framework...

[10/13/25 10:52:53] INFO     Registry: Loaded application registry: bolt
                    INFO     Registry: Built merged registry with 1 applications
                    INFO     Registry: Initializing registry system...
                    INFO     Registry: Registered 10 context classes
                    INFO     Memory manager initialized with directory:
                             /Users/xiaoyachong/Documents/3RSE_finish/17bolt/alpha_berkeley/_agent_data/user_memory
                    INFO     Registry: Registered 1 data sources
                    INFO     Registry: Registered 5 core nodes
                    INFO     Registry: Registered 1 services
                    INFO     Registry: Registered 12 capabilities
                    INFO     Registry: Successfully loaded 9/9 custom prompt builders for framework_defaults
                    INFO     Registry: Set default framework prompt provider to: framework_defaults
                    INFO     Registry: Registered 1 framework prompt providers
                    INFO     Registry: Registry initialization complete!
                                Components loaded:
                                   â€¢ 12 capabilities: memory, time_range_parsing, python, respond, clarify,
                             motor_position_read, motor_position_set, detector_image_capture,
                             photogrammetry_scan_execute, reconstruct_object, ply_quality_assessment, display_object
                                   â€¢ 17 nodes (including 5 core infrastructure)
                                   â€¢ 10 context types: MEMORY_CONTEXT, TIME_RANGE, PYTHON_RESULTS, MOTOR_POSITION,
                             MOTOR_MOVEMENT, DETECTOR_IMAGE, PHOTOGRAMMETRY_SCAN, RECONSTRUCT_OBJECT,
                             PLY_QUALITY_ASSESSMENT, DISPLAY_OBJECT
                                   â€¢ 1 data sources: core_user_memory
                                   â€¢ 1 services: python_executor
                    INFO     Registry: Registry export saved to:
                             /Users/xiaoyachong/Documents/3RSE_finish/17bolt/alpha_berkeley/_agent_data/registry_exports/registry_export.json
                    INFO     Registry: Export contains: 12 capabilities, 10 context types
                    INFO     Builder: Creating framework graph using registry components
                    INFO     Builder: Using provided InMemorySaver checkpointer
                    INFO     Builder: Building graph with 17 nodes from registry
                    INFO     âœ… Builder: Successfully created async framework graph with 17 nodes and InMemorySaver
                             checkpointing enabled (R&D mode)
                    INFO     Gateway: Gateway initialized

âœ… Framework initialized! Thread ID: cli_session_423f97fe
  â€¢ Use â†‘/â†“ arrow keys to navigate command history
  â€¢ Use â†/â†’ arrow keys to edit current line
  â€¢ Press Ctrl+L to clear screen
  â€¢ Type 'bye' or 'end' to exit, or press Ctrl+C


COMMAND EXECUTION: "read the motor position"
================================================================================

USER INPUT
--------------------------------------------------------------------------------
ðŸ‘¤ You: read the motor position
ðŸ”„ Processing: read the motor position


TASK PROCESSING PIPELINE
--------------------------------------------------------------------------------

[10/13/25 10:53:04] INFO     Gateway: Processing message: 'read the motor position...'
                    INFO     Gateway: Processing as new conversation turn
                    INFO     Gateway: DEBUG: Fresh state created with 0 execution records
                    INFO     Gateway: Created fresh state for new conversation turn
ðŸ”„ Starting new conversation turn (execution_step_results: 0 records)...


STEP 1: ROUTING TO TASK EXTRACTION
--------------------------------------------------------------------------------
                    INFO     Router: No current task extracted, routing to task extraction
                    INFO     Task_Extraction: Starting Task Extraction and Processing
                    INFO     Registered data source: core_user_memory
                    INFO     Loaded 1 data sources from registry
                    INFO     No data sources available for current context
                    INFO     Task_Extraction: Retrieved data from 0 sources
ðŸ”„ Extracting actionable task from conversation

[10/13/25 10:53:06] INFO     Task_Extraction:  * Extracted: 'Read the motor position...'
                    INFO     Task_Extraction:  * Builds on previous context: False
                    INFO     Task_Extraction:  * Uses memory context: False
                    INFO     âœ… Task_Extraction: Completed Task Extraction and Processing in 2.13s
ðŸ”„ Task extraction completed


STEP 2: TASK CLASSIFICATION
--------------------------------------------------------------------------------
                    INFO     Router: No active capabilities, routing to classifier
                    INFO     Classifier: Starting Task Classification and Capability Selection
                    INFO     Classifier: Analyzing task requirements...
                    INFO     Classifier: Classifying task: Read the motor position
ðŸ”„ Analyzing task requirements...

[10/13/25 10:53:08] INFO     Classifier:  >>> Capability 'memory' >>> False
[10/13/25 10:53:10] INFO     Classifier:  >>> Capability 'time_range_parsing' >>> False
[10/13/25 10:53:17] INFO     Classifier:  >>> Capability 'python' >>> True
[10/13/25 10:53:19] INFO     Classifier:  >>> Capability 'motor_position_read' >>> True
[10/13/25 10:53:21] INFO     Classifier:  >>> Capability 'motor_position_set' >>> False
[10/13/25 10:53:22] INFO     Classifier:  >>> Capability 'detector_image_capture' >>> False
[10/13/25 10:53:24] INFO     Classifier:  >>> Capability 'photogrammetry_scan_execute' >>> False
[10/13/25 10:53:26] INFO     Classifier:  >>> Capability 'reconstruct_object' >>> False
[10/13/25 10:53:28] INFO     Classifier:  >>> Capability 'ply_quality_assessment' >>> False
[10/13/25 10:53:30] INFO     Classifier:  >>> Capability 'display_object' >>> False

                    INFO     Classifier: 4 capabilities required: ['respond', 'clarify', 'python',
                             'motor_position_read']
                    INFO     Classifier: Classification completed with 4 active capabilities
                    INFO     Classifier: Classification completed
                    INFO     âœ… Classifier: Completed Task Classification and Capability Selection in 24.21s
ðŸ”„ Task classification complete


STEP 3: ORCHESTRATION & EXECUTION PLANNING
--------------------------------------------------------------------------------
                    INFO     Router: No execution plan, routing to orchestrator
                    INFO     Orchestrator: Starting Execution Planning and Orchestration
                    INFO     Orchestrator: Planning for task: Read the motor position
                    INFO     Orchestrator: Available capabilities: ['respond', 'clarify', 'python',
                             'motor_position_read']
                    INFO     Orchestrator: Creating orchestrator prompt for task: "Read the motor position..."
                    INFO     Orchestrator: Active capabilities: ['respond', 'clarify', 'python', 'motor_position_read']
                    INFO     Orchestrator: Constructed orchestrator instructions using:
                    INFO     Orchestrator:  - 4 capabilities
                    INFO     Orchestrator:  - 7 structured examples
                    INFO     Orchestrator:  - 0 context types
                    INFO     Orchestrator: Creating execution plan with orchestrator LLM
ðŸ”„ Generating execution plan...

[10/13/25 10:53:41] INFO     Orchestrator: Orchestrator LLM execution time: 10.55 seconds
                    INFO     Orchestrator: ==================================================
                    INFO     Orchestrator:  << Step 1
                    INFO     Orchestrator:  << â”œâ”€â”€â”€â”€â”€ id: 'current_motor_position'
                    INFO     Orchestrator:  << â”œâ”€â”€â”€ node: 'motor_position_read'
                    INFO     Orchestrator:  << â”œâ”€â”€â”€ task: 'Read the current position of the sample rotation motor to
                             determine its angle in degrees'
                    INFO     Orchestrator:  << â””â”€ inputs: '[]'
                    INFO     Orchestrator:  << Step 2
                    INFO     Orchestrator:  << â”œâ”€â”€â”€â”€â”€ id: 'motor_position_response'
                    INFO     Orchestrator:  << â”œâ”€â”€â”€ node: 'respond'
                    INFO     Orchestrator:  << â”œâ”€â”€â”€ task: 'Provide the user with the current motor position information
                             including motor ID, angle in degrees, and timestamp'
                    INFO     Orchestrator:  << â””â”€ inputs: '[{'MOTOR_POSITION': 'current_motor_position'}]'
                    INFO     Orchestrator: ==================================================
                    INFO     âœ… Orchestrator: Final execution plan ready with 2 steps
                    INFO     Orchestrator: Planning mode not enabled - proceeding with normal execution
                    INFO     Orchestrator: Orchestration processing completed
                    INFO     âœ… Orchestrator: Completed Execution Planning and Orchestration in 10.59s
ðŸ”„ Execution plan created


STEP 4: EXECUTE PLAN - STEP 1 (motor_position_read)
--------------------------------------------------------------------------------
                    INFO     Router: Executing step 1/2 - capability: motor_position_read
                    INFO     Motor_Position_Read: Executing capability: motor_position_read
[10/13/25 10:53:43] INFO     Motor_Position_Read: State updates: step 1
ðŸ”„ Executing motor_position_read... (10%)
ðŸ”„ Reading motor position...
ðŸ”„ Reading position from DMC01:A...
ðŸ”„ Motor position read: DMC01:A at 49.99921875Â°


STEP 5: EXECUTE PLAN - STEP 2 (respond)
--------------------------------------------------------------------------------
                    INFO     Router: Executing step 2/2 - capability: respond
                    INFO     Respond: Executing capability: respond
                    INFO     Message_Generator: Using technical response mode (context type: specific_context)
ðŸ”„ Executing respond... (10%)
ðŸ”„ Gathering information for response...
ðŸ”„ Generating response...

[10/13/25 10:53:45] INFO     Respond: Generated response for: 'Provide the user with the current motor position
                             information including motor ID, angle in degrees, and timestamp'
[10/13/25 10:53:46] INFO     Respond: State updates: step 2
ðŸ”„ Response generated


EXECUTION COMPLETE
--------------------------------------------------------------------------------
ðŸ“Š Execution completed (execution_step_results: 2 records)

ðŸ¤– The current position of Motor DMC01:A is 49.99921875Â°. This data was retrieved from Tiled data after execution through the queue server on 2025-10-13 at 10:53.

ðŸ‘¤ You:

ðŸ‘‹ Goodbye!


================================================================================
PROFILING ANALYSIS RESULTS
================================================================================

ðŸ“Š Analyzing results...


TOP 30 SLOWEST FUNCTIONS
--------------------------------------------------------------------------------
Mon Oct 13 10:54:03 2025    bolt_profile_20251013_105251.prof

         4942082 function calls (4728733 primitive calls) in 71.517 seconds

   Ordered by: cumulative time
   List reduced from 14326 to 30 due to restriction <30>

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
   3641/1    0.063    0.000   71.530   71.530 {built-in method builtins.exec}
        1    0.000    0.000   71.530   71.530 interfaces/CLI/direct_conversation.py:1(<module>)
        1    0.000    0.000   69.728   69.728 /Users/xiaoyachong/anaconda3/envs/alpha/lib/python3.11/asyncio/runners.py:160(run)
        3    0.000    0.000   69.727   23.242 /Users/xiaoyachong/anaconda3/envs/alpha/lib/python3.11/asyncio/base_events.py:614(run_until_complete)
        3    0.001    0.000   69.727   23.242 /Users/xiaoyachong/anaconda3/envs/alpha/lib/python3.11/asyncio/base_events.py:591(run_forever)
        1    0.000    0.000   69.726   69.726 /Users/xiaoyachong/anaconda3/envs/alpha/lib/python3.11/asyncio/runners.py:86(run)
      470    0.011    0.000   69.726    0.148 /Users/xiaoyachong/anaconda3/envs/alpha/lib/python3.11/asyncio/base_events.py:1834(_run_once)
      470    0.005    0.000   66.768    0.142 /Users/xiaoyachong/anaconda3/envs/alpha/lib/python3.11/selectors.py:553(select)
      476   66.761    0.140   66.761    0.140 {method 'control' of 'select.kqueue' objects}
     1085    0.002    0.000    2.942    0.003 /Users/xiaoyachong/anaconda3/envs/alpha/lib/python3.11/asyncio/events.py:78(_run)
1173/1085    0.004    0.000    2.940    0.003 {method 'run' of '_contextvars.Context' objects}
    64/49    0.000    0.000    2.473    0.050 /Users/xiaoyachong/anaconda3/envs/alpha/lib/python3.11/site-packages/langgraph/_internal/_runnable.py:406(ainvoke)
        3    0.000    0.000    2.372    0.791 /Users/xiaoyachong/Documents/3RSE_finish/17bolt/alpha_berkeley/interfaces/CLI/../../src/framework/base/decorators.py:194(langgraph_node)
        1    0.000    0.000    2.356    2.356 /Users/xiaoyachong/Documents/3RSE_finish/17bolt/alpha_berkeley/interfaces/CLI/../../src/applications/bolt/capabilities/motor_position_read.py:38(execute)
        1    0.001    0.001    2.355    2.355 /Users/xiaoyachong/Documents/3RSE_finish/17bolt/alpha_berkeley/interfaces/CLI/../../src/applications/bolt/bolt_api.py:83(get_current_angle)
  2601/32    0.006    0.000    1.947    0.061 <frozen importlib._bootstrap>:1167(_find_and_load)
  2551/29    0.006    0.000    1.947    0.067 <frozen importlib._bootstrap>:1122(_find_and_load_unlocked)
  2466/33    0.006    0.000    1.943    0.059 <frozen importlib._bootstrap>:666(_load_unlocked)
  2381/30    0.003    0.000    1.943    0.065 <frozen importlib._bootstrap_external>:934(exec_module)
  5379/59    0.002    0.000    1.938    0.033 <frozen importlib._bootstrap>:233(_call_with_frames_removed)
   410/99    0.000    0.000    1.643    0.017 {built-in method builtins.__import__}
2466/1154    0.002    0.000    1.296    0.001 <frozen importlib._bootstrap>:1209(_handle_fromlist)
6744/5434    0.038    0.000    1.229    0.000 {built-in method builtins.__build_class__}
        1    0.000    0.000    1.044    1.044 /Users/xiaoyachong/Documents/3RSE_finish/17bolt/alpha_berkeley/interfaces/CLI/../../src/framework/infrastructure/gateway.py:1(<module>)
        1    0.000    0.000    1.043    1.043 /Users/xiaoyachong/Documents/3RSE_finish/17bolt/alpha_berkeley/interfaces/CLI/../../src/framework/models/__init__.py:1(<module>)
        1    1.004    1.004    1.004    1.004 {built-in method time.sleep}
        1    0.000    0.000    0.969    0.969 /Users/xiaoyachong/Documents/3RSE_finish/17bolt/alpha_berkeley/interfaces/CLI/../../src/framework/models/factory.py:1(<module>)
        3    0.000    0.000    0.943    0.314 /Users/xiaoyachong/anaconda3/envs/alpha/lib/python3.11/subprocess.py:505(run)
     1049    0.012    0.000    0.914    0.001 /Users/xiaoyachong/anaconda3/envs/alpha/lib/python3.11/site-packages/pydantic/_internal/_model_construction.py:80(__new__)
        3    0.000    0.000    0.909    0.303 /Users/xiaoyachong/anaconda3/envs/alpha/lib/python3.11/subprocess.py:1161(communicate)


GATEWAY & INFRASTRUCTURE
--------------------------------------------------------------------------------
Mon Oct 13 10:54:03 2025    bolt_profile_20251013_105251.prof

         4942082 function calls (4728733 primitive calls) in 71.517 seconds

   Ordered by: cumulative time
   List reduced from 14326 to 40 due to restriction <'gateway|classification|orchestration'>
   List reduced from 40 to 20 due to restriction <20>

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        1    0.000    0.000    1.044    1.044 /Users/xiaoyachong/Documents/3RSE_finish/17bolt/alpha_berkeley/interfaces/CLI/../../src/framework/infrastructure/gateway.py:1(<module>)
       11    0.000    0.000    0.043    0.004 /Users/xiaoyachong/Documents/3RSE_finish/17bolt/alpha_berkeley/interfaces/CLI/../../src/framework/infrastructure/classification_node.py:136(execute)
        2    0.000    0.000    0.039    0.019 /Users/xiaoyachong/Documents/3RSE_finish/17bolt/alpha_berkeley/interfaces/CLI/../../src/framework/infrastructure/orchestration_node.py:218(execute)
       11    0.000    0.000    0.037    0.003 /Users/xiaoyachong/Documents/3RSE_finish/17bolt/alpha_berkeley/interfaces/CLI/../../src/framework/infrastructure/classification_node.py:240(select_capabilities)
       20    0.000    0.000    0.036    0.002 /Users/xiaoyachong/Documents/3RSE_finish/17bolt/alpha_berkeley/interfaces/CLI/../../src/framework/infrastructure/classification_node.py:285(_classify_capability)
        1    0.000    0.000    0.021    0.021 /Users/xiaoyachong/Documents/3RSE_finish/17bolt/alpha_berkeley/interfaces/CLI/../../src/framework/infrastructure/orchestration_node.py:484(_log_execution_plan)
        1    0.000    0.000    0.008    0.008 /Users/xiaoyachong/Documents/3RSE_finish/17bolt/alpha_berkeley/interfaces/CLI/../../src/framework/infrastructure/gateway.py:110(process_message)
        1    0.000    0.000    0.008    0.008 /Users/xiaoyachong/Documents/3RSE_finish/17bolt/alpha_berkeley/interfaces/CLI/../../src/framework/infrastructure/orchestration_node.py:332(create_system_prompt)
       10    0.000    0.000    0.007    0.001 /Users/xiaoyachong/Documents/3RSE_finish/17bolt/alpha_berkeley/interfaces/CLI/../../src/framework/prompts/defaults/classification.py:57(get_system_instructions)
        1    0.000    0.000    0.004    0.004 /Users/xiaoyachong/Documents/3RSE_finish/17bolt/alpha_berkeley/interfaces/CLI/../../src/framework/infrastructure/gateway.py:197(_handle_new_message_flow)
        1    0.000    0.000    0.001    0.001 /Users/xiaoyachong/Documents/3RSE_finish/17bolt/alpha_berkeley/interfaces/CLI/../../src/framework/infrastructure/gateway.py:92(__init__)
       10    0.000    0.000    0.000    0.000 /Users/xiaoyachong/Documents/3RSE_finish/17bolt/alpha_berkeley/interfaces/CLI/../../src/framework/prompts/defaults/classification.py:22(get_instructions)
        1    0.000    0.000    0.000    0.000 /Users/xiaoyachong/anaconda3/envs/alpha/lib/python3.11/site-packages/anthropic/types/shared/gateway_timeout_error.py:1(<module>)
        1    0.000    0.000    0.000    0.000 /Users/xiaoyachong/anaconda3/envs/alpha/lib/python3.11/site-packages/anthropic/types/beta_gateway_timeout_error.py:1(<module>)
        1    0.000    0.000    0.000    0.000 /Users/xiaoyachong/Documents/3RSE_finish/17bolt/alpha_berkeley/interfaces/CLI/../../src/framework/infrastructure/gateway.py:249(_has_pending_interrupts)
        1    0.000    0.000    0.000    0.000 /Users/xiaoyachong/Documents/3RSE_finish/17bolt/alpha_berkeley/interfaces/CLI/../../src/framework/prompts/defaults/classification.py:1(<module>)
       10    0.000    0.000    0.000    0.000 /Users/xiaoyachong/Documents/3RSE_finish/17bolt/alpha_berkeley/interfaces/CLI/../../src/framework/prompts/defaults/classification.py:30(_get_dynamic_context)
        1    0.000    0.000    0.000    0.000 /Users/xiaoyachong/Documents/3RSE_finish/17bolt/alpha_berkeley/interfaces/CLI/../../src/framework/infrastructure/orchestration_node.py:1(<module>)
        1    0.000    0.000    0.000    0.000 /Users/xiaoyachong/Documents/3RSE_finish/17bolt/alpha_berkeley/interfaces/CLI/../../src/framework/infrastructure/classification_node.py:1(<module>)
        1    0.000    0.000    0.000    0.000 /Users/xiaoyachong/Documents/3RSE_finish/17bolt/alpha_berkeley/interfaces/CLI/../../src/framework/prompts/defaults/classification.py:9(DefaultClassificationPromptBuilder)


BOLT CAPABILITIES
--------------------------------------------------------------------------------
Mon Oct 13 10:54:03 2025    bolt_profile_20251013_105251.prof

         4942082 function calls (4728733 primitive calls) in 71.517 seconds

   Ordered by: cumulative time
   List reduced from 14326 to 24 due to restriction <'bolt/capabilities'>
   List reduced from 24 to 20 due to restriction <20>

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        1    0.000    0.000    2.356    2.356 /Users/xiaoyachong/Documents/3RSE_finish/17bolt/alpha_berkeley/interfaces/CLI/../../src/applications/bolt/capabilities/motor_position_read.py:38(execute)
        1    0.000    0.000    0.003    0.003 /Users/xiaoyachong/Documents/3RSE_finish/17bolt/alpha_berkeley/interfaces/CLI/../../src/applications/bolt/capabilities/__init__.py:1(<module>)
        1    0.000    0.000    0.002    0.002 /Users/xiaoyachong/Documents/3RSE_finish/17bolt/alpha_berkeley/interfaces/CLI/../../src/applications/bolt/capabilities/motor_position_read.py:1(<module>)
        1    0.000    0.000    0.000    0.000 /Users/xiaoyachong/Documents/3RSE_finish/17bolt/alpha_berkeley/interfaces/CLI/../../src/applications/bolt/capabilities/motor_position_set.py:176(_create_classifier_guide)
        1    0.000    0.000    0.000    0.000 /Users/xiaoyachong/Documents/3RSE_finish/17bolt/alpha_berkeley/interfaces/CLI/../../src/applications/bolt/capabilities/detector_image_capture.py:147(_create_classifier_guide)
        1    0.000    0.000    0.000    0.000 /Users/xiaoyachong/Documents/3RSE_finish/17bolt/alpha_berkeley/interfaces/CLI/../../src/applications/bolt/capabilities/motor_position_set.py:1(<module>)
        1    0.000    0.000    0.000    0.000 /Users/xiaoyachong/Documents/3RSE_finish/17bolt/alpha_berkeley/interfaces/CLI/../../src/applications/bolt/capabilities/photogrammetry_scan_execute.py:1(<module>)
        1    0.000    0.000    0.000    0.000 /Users/xiaoyachong/Documents/3RSE_finish/17bolt/alpha_berkeley/interfaces/CLI/../../src/applications/bolt/capabilities/reconstruct_object.py:1(<module>)
        1    0.000    0.000    0.000    0.000 /Users/xiaoyachong/Documents/3RSE_finish/17bolt/alpha_berkeley/interfaces/CLI/../../src/applications/bolt/capabilities/detector_image_capture.py:1(<module>)
        1    0.000    0.000    0.000    0.000 /Users/xiaoyachong/Documents/3RSE_finish/17bolt/alpha_berkeley/interfaces/CLI/../../src/applications/bolt/capabilities/display_object.py:1(<module>)
        1    0.000    0.000    0.000    0.000 /Users/xiaoyachong/Documents/3RSE_finish/17bolt/alpha_berkeley/interfaces/CLI/../../src/applications/bolt/capabilities/ply_quality_assessment.py:1(<module>)
        1    0.000    0.000    0.000    0.000 /Users/xiaoyachong/Documents/3RSE_finish/17bolt/alpha_berkeley/interfaces/CLI/../../src/applications/bolt/capabilities/photogrammetry_scan_execute.py:236(_create_classifier_guide)
        1    0.000    0.000    0.000    0.000 /Users/xiaoyachong/Documents/3RSE_finish/17bolt/alpha_berkeley/interfaces/CLI/../../src/applications/bolt/capabilities/reconstruct_object.py:181(_create_classifier_guide)
        1    0.000    0.000    0.000    0.000 /Users/xiaoyachong/Documents/3RSE_finish/17bolt/alpha_berkeley/interfaces/CLI/../../src/applications/bolt/capabilities/motor_position_read.py:149(_create_classifier_guide)
        1    0.000    0.000    0.000    0.000 /Users/xiaoyachong/Documents/3RSE_finish/17bolt/alpha_berkeley/interfaces/CLI/../../src/applications/bolt/capabilities/ply_quality_assessment.py:177(_create_classifier_guide)
        1    0.000    0.000    0.000    0.000 /Users/xiaoyachong/Documents/3RSE_finish/17bolt/alpha_berkeley/interfaces/CLI/../../src/applications/bolt/capabilities/display_object.py:163(_create_classifier_guide)
        1    0.000    0.000    0.000    0.000 /Users/xiaoyachong/Documents/3RSE_finish/17bolt/alpha_berkeley/interfaces/CLI/../../src/applications/bolt/capabilities/motor_position_read.py:108(_create_orchestrator_guide)
        1    0.000    0.000    0.000    0.000 /Users/xiaoyachong/Documents/3RSE_finish/17bolt/alpha_berkeley/interfaces/CLI/../../src/applications/bolt/capabilities/motor_position_set.py:28(MotorPositionSetCapability)
        1    0.000    0.000    0.000    0.000 /Users/xiaoyachong/Documents/3RSE_finish/17bolt/alpha_berkeley/interfaces/CLI/../../src/applications/bolt/capabilities/photogrammetry_scan_execute.py:28(PhotogrammetryScanExecuteCapability)
        1    0.000    0.000    0.000    0.000 /Users/xiaoyachong/Documents/3RSE_finish/17bolt/alpha_berkeley/interfaces/CLI/../../src/applications/bolt/capabilities/motor_position_read.py:28(MotorPositionReadCapability)


API CALLS (HTTP Requests)
--------------------------------------------------------------------------------
Mon Oct 13 10:54:03 2025    bolt_profile_20251013_105251.prof

         4942082 function calls (4728733 primitive calls) in 71.517 seconds

   Ordered by: cumulative time
   List reduced from 14326 to 130 due to restriction <'bolt_api|requests'>
   List reduced from 130 to 15 due to restriction <15>

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        1    0.001    0.001    2.355    2.355 /Users/xiaoyachong/Documents/3RSE_finish/17bolt/alpha_berkeley/interfaces/CLI/../../src/applications/bolt/bolt_api.py:83(get_current_angle)
        1    0.000    0.000    0.060    0.060 /Users/xiaoyachong/anaconda3/envs/alpha/lib/python3.11/site-packages/google/auth/transport/requests.py:1(<module>)
        1    0.000    0.000    0.055    0.055 /Users/xiaoyachong/anaconda3/envs/alpha/lib/python3.11/site-packages/requests/__init__.py:1(<module>)
        1    0.000    0.000    0.015    0.015 /Users/xiaoyachong/anaconda3/envs/alpha/lib/python3.11/site-packages/requests/exceptions.py:1(<module>)
        1    0.000    0.000    0.015    0.015 /Users/xiaoyachong/anaconda3/envs/alpha/lib/python3.11/site-packages/requests/compat.py:1(<module>)
        1    0.000    0.000    0.007    0.007 /Users/xiaoyachong/anaconda3/envs/alpha/lib/python3.11/site-packages/requests/api.py:1(<module>)
        1    0.000    0.000    0.007    0.007 /Users/xiaoyachong/anaconda3/envs/alpha/lib/python3.11/site-packages/requests/sessions.py:1(<module>)
        1    0.000    0.000    0.007    0.007 /Users/xiaoyachong/anaconda3/envs/alpha/lib/python3.11/site-packages/requests/adapters.py:1(<module>)
        1    0.000    0.000    0.007    0.007 /Users/xiaoyachong/anaconda3/envs/alpha/lib/python3.11/site-packages/requests/compat.py:18(_resolve_char_detection)
        1    0.000    0.000    0.003    0.003 /Users/xiaoyachong/anaconda3/envs/alpha/lib/python3.11/site-packages/starlette/requests.py:1(<module>)
        1    0.000    0.000    0.002    0.002 /Users/xiaoyachong/anaconda3/envs/alpha/lib/python3.11/site-packages/requests/utils.py:1(<module>)
        1    0.000    0.000    0.002    0.002 /Users/xiaoyachong/anaconda3/envs/alpha/lib/python3.11/site-packages/requests_toolbelt/__init__.py:1(<module>)
        1    0.000    0.000    0.002    0.002 /Users/xiaoyachong/anaconda3/envs/alpha/lib/python3.11/site-packages/requests/packages.py:1(<module>)
        1    0.000    0.000    0.001    0.001 /Users/xiaoyachong/Documents/3RSE_finish/17bolt/alpha_berkeley/interfaces/CLI/../../src/applications/bolt/bolt_api.py:1(<module>)
        1    0.000    0.000    0.001    0.001 /Users/xiaoyachong/anaconda3/envs/alpha/lib/python3.11/site-packages/requests/models.py:1(<module>)


================================================================================
PROFILING SUMMARY
================================================================================

âœ… Profile saved: bolt_profile_20251013_105251.prof
ðŸ’¡ View interactively: python -m pstats bolt_profile_20251013_105251.prof


KEY FINDINGS
--------------------------------------------------------------------------------

TOTAL EXECUTION TIME: 71.517 seconds

BREAKDOWN:
â”œâ”€ Network I/O (kqueue waiting):     66.761s  (93.3%)
â”œâ”€ Python code execution:            ~4.8s    (6.7%)
â””â”€ Key operations:
   â€¢ Task Extraction:                2.13s
   â€¢ Classification:                 24.21s (wall clock) / 0.043s (CPU)
   â€¢ Orchestration:                  10.59s (wall clock) / 0.039s (CPU)
   â€¢ Motor API call:                 2.36s
   â€¢ Module imports:                 ~1.9s

BOTTLENECK ANALYSIS:
1. Classification node:    24.21s wall time, 0.043s CPU time
   - 10 capability checks Ã— ~2s per LLM API call
   - 93% of time spent waiting for network responses

2. Orchestration node:     10.59s wall time, 0.039s CPU time
   - Single LLM call for execution planning
   - Nearly all time spent in network I/O

3. Motor position read:    2.36s (HTTP API call to BOLT hardware)

TOTAL LLM WAITING TIME:    ~35 seconds (81% of total execution)
```